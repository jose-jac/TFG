{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recogemos\tlos datos de la base de datos\n",
    "df_clean = pd.read_csv('../data/train_clean.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ponderación con Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizamos el texto\n",
    "df_clean_tokens = pd.DataFrame()\n",
    "df_clean_tokens['text'] = df_clean['text'].apply(nltk.word_tokenize)\n",
    "df_clean_tokens['label'] = df_clean['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para entrenar el modelo Naive Bayes\n",
    "def entrenar_naive_bayes(df_train):\n",
    "\tfrecuencias_true = defaultdict(int)\n",
    "\tfrecuencias_fake = defaultdict(int)\n",
    "\n",
    "\tfor index, noticia in df_train.iterrows():\n",
    "\t\tes_verdadera = noticia['label'] == 0\n",
    "\t\tpalabras = set(noticia['text'])  # Utilizamos un conjunto para evitar contar varias veces la misma palabra en una noticia\n",
    "\t\t\n",
    "\t\tfor palabra in palabras:\n",
    "\t\t\tif es_verdadera:\n",
    "\t\t\t\tfrecuencias_true[palabra] += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tfrecuencias_fake[palabra] += 1\n",
    "\n",
    "\t# Crear un DataFrame con las frecuencias de palabras en noticias verdaderas\n",
    "\tdf_true = pd.DataFrame({'Palabra': list(frecuencias_true.keys()), 'Frecuencia_true': list(frecuencias_true.values())})\n",
    "\n",
    "\t# Crear un DataFrame con las frecuencias de palabras en noticias falsas\n",
    "\tdf_fake = pd.DataFrame({'Palabra': list(frecuencias_fake.keys()), 'Frecuencia_fake': list(frecuencias_fake.values())})\n",
    "\n",
    "\t# Combinar los DataFrames por la columna 'Palabra'\n",
    "\tdf_frecs = pd.merge(df_true, df_fake, on='Palabra', how='outer').fillna(0)\n",
    " \n",
    "\t# Noticias de cada tipo\n",
    "\tn_true = len(df_train[df_train['label'] == 0])\n",
    "\tn_fake = len(df_train[df_train['label'] == 1])\n",
    "\n",
    "\t# Calculamos las probabilidades de si una noticia es verdadera o falsa\n",
    "\tprob_true = n_true / (n_true + n_fake)\n",
    "\tprob_fake = n_fake / (n_true + n_fake)\n",
    " \n",
    "\t# Crear un diccionario para almacenar las probabilidades condicionales de cada palabra\n",
    "\tp_word_given_true = {}\n",
    "\tp_word_given_fake = {}\n",
    "\n",
    "\t# Calcular las probabilidades condicionales de cada palabra\n",
    "\tfor word in df_frecs['Palabra']:\n",
    "\t\t# Calcular la frecuencia de la palabra en noticias verdaderas y falsas\n",
    "\t\tfreq_true = frecuencias_true.get(word, 0)\n",
    "\t\tfreq_fake = frecuencias_fake.get(word, 0)\n",
    "\n",
    "\t\t# Calcular la probabilidad condicional de que la palabra aparezca en una noticia verdadera o falsa\n",
    "\t\tp_word_given_true[word] = (freq_true + 1) / (n_true + 2)  # Laplace smoothing\n",
    "\t\tp_word_given_fake[word] = (freq_fake + 1) / (n_fake + 2)  # Laplace smoothing\n",
    "  \n",
    "\treturn p_word_given_true, p_word_given_fake, prob_true, prob_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para predecir la clase de una noticia\n",
    "def prediccion(conjunto_test, diccionario_true, diccionario_fake, probabilidad_true, probabilidad_fake, alpha=1):\n",
    "    predicciones = []\n",
    "    for index, noticia in conjunto_test.iterrows():\n",
    "        # Inicializar las probabilidades de que la noticia sea verdadera o falsa\n",
    "        p_news_given_true = math.log(probabilidad_true)\n",
    "        p_news_given_fake = math.log(probabilidad_fake)\n",
    "\n",
    "        words = set(noticia['text'])\n",
    "        \n",
    "        # Calcular la probabilidad de la noticia dado que sea verdadera\n",
    "        for word in words:\n",
    "            if word in diccionario_true:\n",
    "                p_news_given_true += math.log(diccionario_true[word])\n",
    "            else:\n",
    "                p_news_given_true += math.log(alpha) - math.log(len(diccionario_true) + alpha)\n",
    "\n",
    "        # Calcular la probabilidad de la noticia dado que sea falsa\n",
    "        for word in words:\n",
    "            if word in diccionario_fake:\n",
    "                p_news_given_fake += math.log(diccionario_fake[word])\n",
    "            else:\n",
    "                p_news_given_true += math.log(alpha) - math.log(len(diccionario_true) + alpha)\n",
    "\n",
    "        # Comparar las probabilidades y clasificar la noticia\n",
    "        if p_news_given_true > p_news_given_fake:\n",
    "            predicciones.append(0)\n",
    "        else:\n",
    "            predicciones.append(1)\n",
    "    return predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para evaluar el modelo Naive Bayes\n",
    "def metricas(y_test, predicciones):\n",
    "    # Matriz de confusión\n",
    "\tcm = confusion_matrix(y_test, predicciones)\n",
    "\tcm\n",
    "\n",
    "\t# Extraer los valores de la matriz de confusión\n",
    "\ttp, fn, fp, tn = cm.ravel()\n",
    "\n",
    "\t# Mostramos tp, fp, fn, tn\n",
    "\tprint(\"TP: {}\".format(tp))\n",
    "\tprint(\"FP: {}\".format(fp))\n",
    "\tprint(\"FN: {}\".format(fn))\n",
    "\tprint(\"TN: {}\".format(tn))\n",
    "\n",
    "\t# Calcular las métricas\n",
    "\ttpr = tp / (tp + fn)  # Tasa de verdaderos positivos (Recall)\n",
    "\ttnr = tn / (tn + fp)  # Tasa de verdaderos negativos\n",
    "\tfpr = fp / (fp + tn)  # Tasa de falsos positivos\n",
    "\tfnr = fn / (fn + tp)  # Tasa de falsos negativos\n",
    "\n",
    "\t# Imprimir las métricas\n",
    "\tprint(\"TPR (Recall): {:.2f}\".format(tpr))\n",
    "\tprint(\"TNR: {:.2f}\".format(tnr))\n",
    "\tprint(\"FPR: {:.2f}\".format(fpr))\n",
    "\tprint(\"FNR: {:.2f}\".format(fnr))\n",
    "\n",
    "\t# Comparar las predicciones con las etiquetas verdaderas\n",
    "\taccuracy = accuracy_score(y_test, predicciones)\n",
    "\tprecision = precision_score(y_test, predicciones, pos_label=0)\n",
    "\trecall = recall_score(y_test, predicciones, pos_label=0)\n",
    "\tf1 = f1_score(y_test, predicciones, pos_label=0)\n",
    " \t\n",
    "  \t# Calcular la curva ROC\n",
    "\tfpr, tpr, thresholds = roc_curve(y_test, predicciones)\n",
    "\t\n",
    "\troc_auc = auc(fpr, tpr)\n",
    "\n",
    "\t# Imprimir las métricas de evaluación\n",
    "\tprint(\"Exactitud ((tp + tn) / (tp + tn+fp+ fn)): {:.2f}\".format(accuracy))\n",
    "\tprint(\"Precisión (tp / (tp + fp)): {:.2f}\".format(precision))\n",
    "\tprint(\"Recall (tp / (tp + fn)): {:.2f}\".format(recall))\n",
    "\tprint(\"Puntuación F1 (2 * (precision * recall) / (precision + recall)): {:.2f}\".format(f1))\n",
    "\tprint(\"AUC: {:.2f}\".format(roc_auc))\n",
    "\t\n",
    "\t# Graficar la curva ROC\n",
    "\tplt.figure()\n",
    "\tplt.plot(fpr, tpr, label='Curva ROC (AUC = {:.2f})'.format(roc_auc))\n",
    "\tplt.plot([0, 1], [0, 1], 'k--')  # Línea diagonal para referencia\n",
    "\tplt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "\tplt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "\tplt.title('Curva ROC')\n",
    "\tplt.legend(loc='lower right')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=777)\n",
    "\n",
    "# Obtener los índices de los conjuntos de entrenamiento y prueba para cada fold\n",
    "for train_index, test_index in skf.split(df_clean_tokens['text'], df_clean_tokens['label']):\n",
    "    \n",
    "    # Obtener los conjuntos de entrenamiento y prueba para el fold actual\n",
    "    X_train, X_test = df_clean_tokens.iloc[train_index], df_clean_tokens.iloc[test_index]\n",
    "    y_train, y_test = df_clean_tokens.iloc[train_index]['label'], df_clean_tokens.iloc[test_index]['label']\n",
    "\n",
    "    p_word_given_true, p_word_given_fake, prob_true, prob_fake = entrenar_naive_bayes(X_train)\n",
    "    predicciones = prediccion(X_test, p_word_given_true, p_word_given_fake, prob_true, prob_fake)\n",
    "    metricas(y_test, predicciones)\n",
    "    print(\"--------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee5bf18464c5a0cdd7c8ceb4ffa371b05ef38df8192946373c6ecb0c4072f62a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
