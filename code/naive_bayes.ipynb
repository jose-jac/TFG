{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jabel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recogemos\tlos datos de la base de datos\n",
    "# y los guardamos en un dataframe\n",
    "df = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'author', 'text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar texto\n",
    "def clean_text(text):\n",
    "\t# Eliminar caracteres especiales\n",
    "\ttext = re.sub(r'\\W', ' ', str(text))\n",
    "\n",
    "\t# Eliminar palabras con números\n",
    "\ttext = re.sub(r'\\w*\\d\\w*', ' ', text)\n",
    "\n",
    "\t# Eliminar espacios en blanco\n",
    "\ttext = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "\t# Eliminar stopwords\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttext = text.split()\n",
    "\ttext = [w for w in text if not w in stop_words]\n",
    "\ttext = \" \".join(text)\n",
    "\n",
    "\t# Stemming\n",
    "\ttext = text.split()\n",
    "\tstemmer = SnowballStemmer('english')\n",
    "\tstemmed_words = [stemmer.stem(word) for word in text]\n",
    "\ttext = \" \".join(stemmed_words)\n",
    " \n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hous dem aid we didn even see comey letter until jason chaffetz tweet it\n",
      "flynn hillari clinton big woman campus breitbart\n",
      "whi truth might get you fire\n",
      "civilian kill in singl us airstrik have been identifi\n",
      "iranian woman jail fiction unpublish stori woman stone death adulteri\n",
      "jacki mason hollywood would love trump he bomb north korea lack tran bathroom exclus video breitbart\n",
      "life life of luxuri elton john favorit shark pictur to stare at dure long transcontinent flight\n",
      "benoît hamon win french socialist parti presidenti nomin the new york time\n",
      "excerpt from draft script donald trump q ampa with black church pastor the new york time\n",
      "a back channel plan ukrain russia courtesi trump associ the new york time\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(clean_text(df['title'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20800"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texto limpio en un nuevo dataframe\n",
    "df2 = pd.DataFrame()\n",
    "df2['title'] = df['title'].apply(clean_text)\n",
    "df2['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# tiempo de ejecución => 3 minutos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['label'] = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras totales titulos: 1504372\n",
      "Palabras por titulo: 72.32557692307692\n",
      "Palabras totales noticias: 94518924\n",
      "Palabras por noticia: 4544.179038461539\n"
     ]
    }
   ],
   "source": [
    "# Calculo de palabras a procesar\n",
    "num_palabras_titulos = 0\n",
    "num_palabras_textos = 0\n",
    "for i in range(0,len(df)):\n",
    "\tif(type(df2['text'][i]) == str):\n",
    "\t\tnum_palabras_textos = num_palabras_textos + len(df['text'][i])\n",
    "\tif(type(df2['title'][i]) == str):\n",
    "\t\tnum_palabras_titulos = num_palabras_titulos + len(df['title'][i])\n",
    "print('Palabras totales titulos: ' + str(num_palabras_titulos))\n",
    "print('Palabras por titulo: ' + str(num_palabras_titulos/len(df)))\n",
    "print('Palabras totales noticias: ' + str(num_palabras_textos))\n",
    "print('Palabras por noticia: ' + str(num_palabras_textos/len(df)))\n",
    "\n",
    "# Tiempo aproximado de procesamiento de todas las noticias y titulos de entrenamiento => 25 minutos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ponderación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_text = {}\n",
    "fake_text = {}\n",
    "\n",
    "true = df2[df2['label'] == 0]\n",
    "fake = df2[df2['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_diccionario(clase, diccionario):\n",
    "\tfor titulo in clase['title']:\n",
    "\t\tif(type(titulo) == str):\n",
    "\t\t\tpalabras = titulo.split()\n",
    "\t\t\tfor palabra in palabras:\n",
    "\t\t\t\tif palabra in diccionario:\n",
    "\t\t\t\t\tdiccionario[palabra] += 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdiccionario[palabra] = 1\n",
    "\treturn diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conteo(diccionario):\n",
    "\tlength = 0\n",
    "\tfor key in diccionario:\n",
    "\t\tlength += diccionario[key]\n",
    "\treturn length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conteo_palabras(diccionario, length):\n",
    "    for term in diccionario:\n",
    "        diccionario[term] = diccionario[term]/length\n",
    "    return diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilidades(diccionario, X, initial):\n",
    "    X = clean_text(X)\n",
    "    split = X.split()\n",
    "    probability = initial\n",
    "    for term in split:\n",
    "        if term in diccionario:\n",
    "            probability *= diccionario[term]\n",
    "            #print(term,diccionario[term])\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_text = crear_diccionario(true,true_text)\n",
    "fake_text = crear_diccionario(fake,fake_text)\n",
    "true_count = conteo(true_text)\n",
    "fake_count = conteo(fake_text)\n",
    "true_text = conteo_palabras(true_text,true_count)\n",
    "fake_text = conteo_palabras(fake_text,fake_count)\n",
    "total_count = true_count + fake_count\n",
    "fake_initial = fake_count/total_count\n",
    "true_initial = true_count/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediccion(X):\n",
    "\tif probabilidades(true_text, X, 1) > probabilidades(fake_text, X, 1):\n",
    "\t\treturn True\n",
    "\telse:\n",
    "\t\treturn False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediccion('Trump is potato')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee5bf18464c5a0cdd7c8ceb4ffa371b05ef38df8192946373c6ecb0c4072f62a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
