{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recogemos los datos limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos las noticias limpias en df_clean\n",
    "df_clean = pd.read_csv('../data/train_clean.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizamos el texto\n",
    "df_clean_tokens = pd.DataFrame()\n",
    "df_clean_tokens['text'] = df_clean['text'].apply(nltk.word_tokenize)\n",
    "df_clean_tokens['label'] = df_clean['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el dataset en train y test con la función sample de pandas\n",
    "df_train, df_test = train_test_split(df_clean_tokens, test_size=0.2, random_state=777)\n",
    "\n",
    "print(\"Ejemplos usados para entrenar: \", len(df_train))\n",
    "print(\"Ejemplos usados para test: \", len(df_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etiquetamos texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si label es 0, incluimos en text True, False si es 1\n",
    "for i in range(len(df_train['label'])):\n",
    "\tif df_train['label'].iloc[i] == 0:\n",
    "\t\tdf_train['text'].iloc[i].append(\"TRUE\")\n",
    "\telse:\n",
    "\t\tdf_train['text'].iloc[i].append(\"FALSE\")\n",
    "  \n",
    "df_train = df_train.drop('label', 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos dos word cloud con el texto limpio y tokenizado de cada label\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "\t\t\t\tbackground_color ='white',\n",
    "\t\t\t\tmin_font_size = 10).generate(str(df_clean_tokens['text'][df_clean_tokens['label'] == 0]))\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "\t\t\t\tbackground_color ='white',\n",
    "\t\t\t\tmin_font_size = 10).generate(str(df_clean_tokens['text'][df_clean_tokens['label'] == 1]))\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enseñamos las palabras más frecuentes de cada label\n",
    "def get_top_n_words(corpus, n=None):\n",
    "\tvec = CountVectorizer().fit(corpus)\n",
    "\tbag_of_words = vec.transform(corpus)\n",
    "\tsum_words = bag_of_words.sum(axis=0)\n",
    "\twords_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "\twords_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\treturn words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 palabras más frecuentes de cada label\n",
    "print('Palabras más frecuentes en noticias Verdaderas:')\n",
    "common_words = get_top_n_words(df_clean['text'][df_clean_tokens['label'] == 0], 20)\n",
    "for word, freq in common_words:\n",
    "\tprint(word, freq)\n",
    "print('-------------')\n",
    "print('Palabras más frecuentes en noticias Falsas:')\n",
    "common_words = get_top_n_words(df_clean['text'][df_clean_tokens['label'] == 1], 20)\n",
    "for word, freq in common_words:\n",
    "\tprint(word, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos los datos en train y test\n",
    "p_train = 0.80\n",
    "\n",
    "df_clean_tokens['is_train'] = np.random.uniform(0, 1, len(df)) <= p_train\n",
    "df_train = df_clean_tokens[df_clean_tokens['is_train'] == True]\n",
    "df_test =  df_clean_tokens[df_clean_tokens['is_train'] == False]\n",
    "df_clean_tokens = df_clean_tokens.drop('is_train', 1)\n",
    "df_train = df_train.drop('is_train', 1)\n",
    "df_test = df_test.drop('is_train', 1)\n",
    "\n",
    "print(\"Ejemplos usados para entrenar: \", len(df_train))\n",
    "print(\"Ejemplos usados para test: \", len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso 1: Sin N-Gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparamos los datos para procesar Apriori\n",
    "te1 = TransactionEncoder()\n",
    "te_ary1 = te1.fit(df_train['text']).transform(df_train['text'])\n",
    "df_apriori1 = pd.DataFrame(te_ary1, columns=te1.columns_)\n",
    "df_apriori1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjuntos de items frecuentes\n",
    "itemsets_frecuentes1 = apriori(df_apriori1, min_support=0.2, use_colnames=True)\n",
    "itemsets_frecuentes1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reglas de asociación apriori\n",
    "reglas1 = association_rules(itemsets_frecuentes1, metric=\"confidence\", min_threshold=0.2)\n",
    "reglas1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escogemos las reglas que contienen solamente True o solamente False en el consecuente\n",
    "reglas_VoF1 = reglas1[(reglas1['consequents'] == frozenset({'TRUE'})) | (reglas1['consequents'] == frozenset({'FALSE'}))]\n",
    "reglas_VoF1.sort_values(by=['confidence'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos la confianza maxima de las reglas falsas y verdaderas por separado\n",
    "reglas_F1 = reglas1[reglas1['consequents'] == frozenset({'FALSE'})]\n",
    "max(reglas_F1['confidence'])\n",
    "\n",
    "reglas_V1 = reglas1[reglas1['consequents'] == frozenset({'TRUE'})]\n",
    "max(reglas_V1['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos las 10 reglas que tienen mayor confianza de las falsas\n",
    "reglas_F1.sort_values(by=['confidence'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos las 10 reglas que tienen mayor confianza de las verdaderas\n",
    "reglas_V1.sort_values(by=['confidence'], ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso 2: Con N-Gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos get_grams\n",
    "def get_ngrams(text, n):\n",
    "\tn_grams = ngrams(text, n)\n",
    "\treturn [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# Creamos un nuevo dataframe donde para cada noticia almacenamos los ngramas de 1 a 3 de longitud\n",
    "df_train_ngramas = pd.DataFrame()\n",
    "df_train_ngramas['text'] = df_train['text']\n",
    "df_train_ngramas['ngrams'] = df_train_ngramas['text'].apply(lambda x: get_ngrams(x, 1)) + df_train_ngramas['text'].apply(lambda x: get_ngrams(x, 2)) + df_train_ngramas['text'].apply(lambda x: get_ngrams(x, 3))\n",
    "\n",
    "df_test_ngramas = pd.DataFrame()\n",
    "df_test_ngramas['text'] = df_test['text']\n",
    "df_test_ngramas['label']\t= df_test['label']\n",
    "df_test_ngramas['ngrams'] = df_test_ngramas['text'].apply(lambda x: get_ngrams(x, 1)) + df_test_ngramas['text'].apply(lambda x: get_ngrams(x, 2)) + df_test_ngramas['text'].apply(lambda x: get_ngrams(x, 3))\n",
    "\n",
    "# Descartamos los ngramas que incluyan las palabras 'TRUE' o 'FALSE'\n",
    "df_train_ngramas['ngrams'] = df_train_ngramas['ngrams'].apply(lambda x: [ngram for ngram in x if 'TRUE' not in ngram and 'FALSE' not in ngram])\n",
    "df_test_ngramas['ngrams'] = df_test_ngramas['ngrams'].apply(lambda x: [ngram for ngram in x if 'TRUE' not in ngram and 'FALSE' not in ngram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos de listas ngramas a sets de ngramas, así no los tendremos repetidos\n",
    "for i in range(len(df_train_ngramas['ngrams'])):\n",
    "\tdf_train_ngramas['ngrams'].iloc[i] = set(df_train_ngramas['ngrams'].iloc[i])\n",
    " \n",
    "for i in range(len(df_test_ngramas['ngrams'])):\n",
    "\tdf_test_ngramas['ngrams'].iloc[i] = set(df_test_ngramas['ngrams'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un set con todos los ngramas de train\n",
    "ngramas = []\n",
    "for i in range(len(df_train_ngramas['ngrams'])):\n",
    "\tngramas += df_train_ngramas['ngrams'].iloc[i]\n",
    "ngramas_set = set(ngramas)\n",
    "len(ngramas_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar la frecuencia de cada ngrama en el dataframe con counter\n",
    "ngramas_counter = Counter(ngramas)\n",
    "\n",
    "# Extraemos en una lista los ngramas que aparecen menos de 'umbral' veces\n",
    "umbral = 50\n",
    "ngramas_eliminar = [ngrama for ngrama, freq in ngramas_counter.items() if freq < umbral]\n",
    "\n",
    "# Calculamos cuáles no vamos a eliminar, restando los conjuntos\n",
    "ngramas_no_eliminar = set(ngramas) - set(ngramas_eliminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos el número total de ngramas en cada dataframe\n",
    "print(\"Número total de ngramas en el dataframe original: \", len(ngramas_counter))\n",
    "print(\"Número total de ngramas en el dataframe limpio: \", len(ngramas_no_eliminar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un nuevo dataframe donde eliminamos los ngramas que aparecen menos de 'umbral' veces\n",
    "df_train_ngramas_clean = pd.DataFrame()\n",
    "df_train_ngramas_clean['text'] = df_train_ngramas['text']\n",
    "df_train_ngramas_clean['ngrams'] = df_train_ngramas['ngrams'].apply(lambda x: set(x).intersection(ngramas_no_eliminar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con los ngramas maximales, es decir los ngramas que no están contenidos en otros ngramas\n",
    "ngramas_maximales = set(ngramas_no_eliminar)\n",
    "for ngrama in ngramas_no_eliminar:\n",
    "\tfor ngrama2 in ngramas_no_eliminar:\n",
    "\t\tif ngrama != ngrama2 and ngrama in ngrama2:\n",
    "\t\t\tngramas_maximales.discard(ngrama)\n",
    "   \t\t\tbreak\n",
    "  \n",
    "print(\"Número total de ngramas maximales: \", len(ngramas_maximales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un set que contenga los bigramas incluidos en los ngramas maximales\n",
    "bigramas_maximales = set()\n",
    "for ngrama in ngramas_maximales:\n",
    "\tsplit = ngrama.split()\n",
    "\tif len(split) == 2:\n",
    "\t\tbigramas_maximales.add(ngrama)\n",
    "\telif len(split) > 2:\n",
    "\t\tfor i in range(len(split)-1):\n",
    "\t\t\tbigramas_maximales.add(split[i] + ' ' + split[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una lista de transacciones donde cada transacción es una lista de elementos\n",
    "transactions = df_train_ngramas_clean['text'].tolist()\n",
    "\n",
    "# Creamos un TransactionEncoder y lo ajustamos a la lista de transacciones\n",
    "te = TransactionEncoder()\n",
    "te.fit(transactions)\n",
    "\n",
    "# Transformamos las transacciones en una matriz booleana donde las filas representan las transacciones y las columnas representan los elementos\n",
    "data = te.transform(transactions)\n",
    "\n",
    "# Creamos un nuevo DataFrame a partir de la matriz booleana y las columnas de elementos correspondientes\n",
    "df_encoded = pd.DataFrame(data, columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los itemsets frecuentes\n",
    "freq_itemsets = apriori(df_encoded, min_support=0.25, use_colnames=True, verbose=1)\n",
    "freq_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reglas de asociación\n",
    "reglas = association_rules(freq_itemsets, metric=\"confidence\", min_threshold=0.2)\n",
    "reglas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escogemos las reglas que contienen un elemento en el antecedente y otro en el consecuente, y ese elemento es un unigrama\n",
    "reglas1 = reglas[\t(reglas['antecedents'].apply(lambda x: len(x)) == 1) & (reglas['consequents'].apply(lambda x: len(x)) == 1)\t& (reglas['antecedents'].apply(lambda x: ' ' not in list(x)[0]) & reglas['consequents'].apply(lambda x: ' ' not in list(x)[0]))   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglas1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una función que diga si una cadena de caracteres es un substring de un ngrama maximal\n",
    "def is_in_maximal(cadena):\n",
    "\treturn cadena in bigramas_maximales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escogemos las reglas cuyo antecedente y consecuente conjuntamente pertenecen a algun ngrama maximal\n",
    "reglas2 = reglas1[\treglas1.apply(lambda x: is_in_maximal(list(x['antecedents'])[0] + ' ' + list(x['consequents'])[0]), axis=1)\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las palabras maximales como aquellas que se encuentran en los ngramas maximales\n",
    "palabras_maximales = set()\n",
    "for ngrama in ngramas_maximales:\n",
    "\tsplit = ngrama.split()\n",
    "\tfor palabra in split:\n",
    "\t\tpalabras_maximales.add(palabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escogemos las reglas que contienen solamente True o solamente False en el consecuente, y en el antecedente hay una palabra contenida en un ngrama maximal\n",
    "reglas_V = reglas1[reglas1['consequents'] == frozenset({'TRUE'})]\n",
    "reglas_V = reglas_V[reglas_V.apply(lambda x: list(x['antecedents'])[0] in palabras_maximales, axis=1)]\n",
    "reglas_V.sort_values(by=['confidence'], ascending=False)\n",
    "reglas_F = reglas1[reglas1['consequents'] == frozenset({'FALSE'})]\n",
    "reglas_F = reglas_F[reglas_F.apply(lambda x: list(x['antecedents'])[0] in palabras_maximales, axis=1)]\n",
    "reglas_F.sort_values(by=['confidence'], ascending=False)\n",
    "\n",
    "# Agrupamos las reglas verdaderas y falsas en un solo dataframe\n",
    "reglas_VoF = pd.DataFrame()\n",
    "reglas_VoF = reglas_V.append(reglas_F)\n",
    "reglas_VoF\n",
    "reglas_VoF.sort_values(by=['confidence'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos una lista con los antecedentes de las reglas VoF\n",
    "antecedentes_VoF = reglas_VoF['antecedents'].tolist()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglas_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una función que vea si hay una regla en reglas2 con el antecedente y consecuente que le pasamos\n",
    "def is_in_reglas2(antecedente, consecuente):\n",
    "\treturn (antecedente, consecuente) in reglas2.apply(lambda x: (list(x['antecedents'])[0], list(x['consequents'])[0]), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escogemos las palabras conectadas con True o False\n",
    "palabras_conectadas = set()\n",
    "nuevas = set([list(x)[0] for x in reglas_VoF['antecedents']])\n",
    "while len(nuevas) > 0:\n",
    "    palabras_conectadas_aux = nuevas.copy()\n",
    "    nuevas = set()\n",
    "    for ant in palabras_conectadas_aux:\n",
    "        nuevas.update(set(reglas2[reglas2.apply(lambda x: list(x['consequents'])[0] == ant, axis=1)]['antecedents'].apply(lambda x: list(x)[0]).tolist()))\n",
    "    nuevas.difference_update(palabras_conectadas)\n",
    "    palabras_conectadas.update(nuevas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_conectadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con las reglas donde tanto el antecedente como el consecuente son palabras conectadas\n",
    "reglas3 = reglas2[reglas2.apply(lambda x: list(x['antecedents'])[0] in palabras_conectadas and list(x['consequents'])[0] in palabras_conectadas, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las reglas que forman ciclos al crear una red con las reglas3\n",
    "G = nx.DiGraph()\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reglas3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(reglas_V['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos las 10 reglas que tienen mayor confianza de las verdaderas\n",
    "reglas_V.sort_values(by=['confidence'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reglas_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(reglas_F['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos las 10 reglas que tienen mayor confianza de las falsas\n",
    "reglas_F.sort_values(by=['confidence'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(reglas_F['support'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafo de Asociación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las reglas que tienen una confianza mayor a 0.4\n",
    "reglas_VoF_grafo = reglas_VoF[reglas_VoF['confidence'] > 0.4]\n",
    "reglas_VoF_grafo = reglas_VoF_grafo.head(100)\n",
    "\n",
    "# Creamos un grafo de las reglas, donde los nodos son los elementos y las aristas son las reglas, y el valor de la arista es la confianza\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Añadimos los nodos\n",
    "for i in range(len(reglas3)):\n",
    "\tG.add_node(next(iter(reglas3['antecedents'].iloc[i])))\n",
    "\tG.add_node(next(iter(reglas3['consequents'].iloc[i])))\n",
    "\n",
    "for i in range(len(reglas_VoF_grafo)):\n",
    "\tG.add_node(next(iter(reglas_VoF_grafo['antecedents'].iloc[i])))\n",
    "\tG.add_node(next(iter(reglas_VoF_grafo['consequents'].iloc[i])))\n",
    "\t\n",
    "# Añadimos las aristas con la confianza como peso\n",
    "for i in range(len(reglas3)):\n",
    "\tG.add_edge(next(iter(reglas3['antecedents'].iloc[i])), next(iter(reglas3['consequents'].iloc[i])), weight=reglas3['confidence'].iloc[i].round(2))\n",
    "\n",
    "for i in range(len(reglas_VoF_grafo)):\n",
    "\tG.add_edge(next(iter(reglas_VoF_grafo['antecedents'].iloc[i])), next(iter(reglas_VoF_grafo['consequents'].iloc[i])), weight=reglas_VoF_grafo['confidence'].iloc[i].round(2))\n",
    "\n",
    "# Dibujamos el grafo\n",
    "plt.figure(figsize=(20,20))\n",
    "pos = nx.spring_layout(G, k=0.5)\n",
    "nx.draw(G, pos, with_labels=True, font_size=20, node_size=2000, node_color='lightblue', edge_color='black', width=1, alpha=0.7)\n",
    "edge_labels = nx.get_edge_attributes(G,'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=20)\n",
    "nx.draw_networkx_edges(G, pos, edgelist=G.edges(), edge_color='black', arrows=True, width=1, alpha=0.7, connectionstyle='arc3, rad=0.3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos si el grafo tiene ciclos\n",
    "nx.is_directed_acyclic_graph(G)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo de Pesos de las Reglas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_ngrams.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los pesos de las reglas mediante la confianza y de forma no lineal, para que las reglas con confianza alta tengan un peso mayor, las que tienen confianza 0.5 tengan peso nulo y las que tienen confianza baja tengan un peso negativo\n",
    "reglas_VoF['peso'] = reglas_VoF['confidence'].apply(lambda x: (x - 0.5)**2)\n",
    "\n",
    "# Calculamos los ngramas del conjunto test\n",
    "df_test_ngrams_clean = pd.DataFrame()\n",
    "df_test_ngrams_clean['text'] = df_test_ngrams['text']\n",
    "df_test_ngrams_clean['label'] = df_test_ngrams['label']\n",
    "df_test_ngrams_clean['ngrams'] = df_test_ngrams['ngrams'].apply(lambda x: set(x).intersection(ngramas_no_eliminar))\n",
    "\n",
    "# Ahora que tenemos las reglas y los ngramas del conjunto test, vamos a clasificar cada noticia test según si cumplen o no las reglas y sumando los pesos de las reglas que se cumplen con votación positiva\n",
    "df_test_ngrams_clean['votos'] = df_test_ngrams_clean['ngrams'].apply(lambda x: sum(reglas_VoF[reglas_VoF['antecedents'].isin(x)]['peso']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos False y True por 1 y 0\n",
    "df_test_ngrams_clean['label'] = df_test_ngrams_clean['label'].apply(lambda x: 1 if x == True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos la predicción de cada noticia test\n",
    "df_test_ngrams_clean['prediccion'] = df_test_ngrams_clean['votos'].apply(lambda x: 0 if x > 0 else 1)\n",
    "\n",
    "# Calculamos la precisión\n",
    "accuracy_score(df_test_ngrams_clean['label'], df_test_ngrams_clean['prediccion'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos la matriz de confusión\n",
    "confusion_matrix(df_test_ngrams_clean['label'], df_test_ngrams_clean['prediccion'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee5bf18464c5a0cdd7c8ceb4ffa371b05ef38df8192946373c6ecb0c4072f62a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
