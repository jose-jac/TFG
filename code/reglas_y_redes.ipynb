{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netgraph import Graph\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from nltk.util import ngrams\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "#from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud\n",
    "import math\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recogemos los datos limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv('../data/train_clean.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizamos el texto\n",
    "df_clean_tokens = pd.DataFrame()\n",
    "df_clean_tokens['text'] = df_clean['text'].apply(nltk.word_tokenize)\n",
    "df_clean_tokens['label'] = df_clean['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el dataset en train y test con la función sample de pandas\n",
    "df_train, df_test = train_test_split(df_clean_tokens, test_size=0.2, random_state=777)\n",
    "\n",
    "print(\"Ejemplos usados para entrenar: \", len(df_train))\n",
    "print(\"Ejemplos usados para test: \", len(df_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etiquetamos texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si label es 0, incluimos en text True, False si es 1\n",
    "for i in range(len(df_train['label'])):\n",
    "\tif df_train['label'].iloc[i] == 0:\n",
    "\t\tdf_train['text'].iloc[i].append(\"TRUE\")\n",
    "\telse:\n",
    "\t\tdf_train['text'].iloc[i].append(\"FALSE\")\n",
    "  \n",
    "df_train = df_train.drop('label', 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso 1: Unigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparamos los datos para procesar Apriori\n",
    "te1 = TransactionEncoder()\n",
    "te_ary1 = te1.fit(df_train['text']).transform(df_train['text'])\n",
    "df_apriori1 = pd.DataFrame(te_ary1, columns=te1.columns_)\n",
    "df_apriori1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuencias_true = defaultdict(int)\n",
    "frecuencias_fake = defaultdict(int)\n",
    "\n",
    "for index, noticia in df_train.iterrows():\n",
    "\tes_verdadera = 'TRUE' in noticia['text']  # Comprobamos si la noticia es verdadera o falsa\n",
    "\tpalabras = set(noticia['text'])  # Utilizamos un conjunto para evitar contar varias veces la misma palabra en una noticia\n",
    "\t\n",
    "\tfor palabra in palabras:\n",
    "\t\tif es_verdadera:\n",
    "\t\t\tfrecuencias_true[palabra] += 1\n",
    "\t\telse:\n",
    "\t\t\tfrecuencias_fake[palabra] += 1\n",
    "\n",
    "# Crear un DataFrame con las frecuencias de palabras en noticias verdaderas\n",
    "df_true = pd.DataFrame({'Palabra': list(frecuencias_true.keys()), 'Frecuencia_true': list(frecuencias_true.values())})\n",
    "\n",
    "# Crear un DataFrame con las frecuencias de palabras en noticias falsas\n",
    "df_fake = pd.DataFrame({'Palabra': list(frecuencias_fake.keys()), 'Frecuencia_fake': list(frecuencias_fake.values())})\n",
    "\n",
    "# Combinar los DataFrames por la columna 'Palabra'\n",
    "df_frecs = pd.merge(df_true, df_fake, on='Palabra', how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos en una lista las palabras con frecuencia menor a 50\n",
    "words_less_50 = []\n",
    "for index, noticia in df_frecs.iterrows():\n",
    "\tif noticia['Frecuencia_true'] + noticia['Frecuencia_fake'] < 50:\n",
    "\t\twords_less_50.append(noticia['Palabra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_more_50 = set(df_apriori1.columns.tolist()) - set(words_less_50)\n",
    "df_apriori1 = df_apriori1[words_more_50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjuntos de items frecuentes\n",
    "itemsets_frecuentes1 = apriori(df_apriori1, min_support=0.2, use_colnames=True)\n",
    "itemsets_frecuentes1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsets_frecuentes1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reglas de asociación apriori\n",
    "reglas_unigramas = association_rules(itemsets_frecuentes1, metric=\"confidence\", min_threshold=0.2)\n",
    "reglas_unigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las reglas de asociación ordenadas por confianza y mostramos el top 25\n",
    "reglas_unigramas.sort_values(by='confidence', ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos cuantas reglas hay con confianza entre 0.9 y 1\n",
    "reglas_unigramas[(reglas_unigramas['confidence'] >= 0.9) & (reglas_unigramas['confidence'] <= 1)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escogemos las reglas que contienen solamente True o solamente False en el consecuente\n",
    "reglas_VoF1 = reglas_unigramas[(reglas_unigramas['consequents'] == frozenset({'TRUE'})) | (reglas_unigramas['consequents'] == frozenset({'FALSE'}))]\n",
    "reglas_VoF1.sort_values(by=['confidence'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un dataframe con las reglas que contienen TRUE y otro para FALSE\n",
    "reglas_true1 = reglas_VoF1[reglas_VoF1['consequents'] == frozenset({'TRUE'})]\n",
    "reglas_false1 = reglas_VoF1[reglas_VoF1['consequents'] == frozenset({'FALSE'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos las confianzas maximas y minimas de las reglas que contienen TRUE y FALSE\n",
    "print(\"Confianza máxima de las reglas que contienen TRUE: \", reglas_true1['confidence'].max())\n",
    "print(\"Confianza mínima de las reglas que contienen TRUE: \", reglas_true1['confidence'].min())\n",
    "print(\"Confianza máxima de las reglas que contienen FALSE: \", reglas_false1['confidence'].max())\n",
    "print(\"Confianza mínima de las reglas que contienen FALSE: \", reglas_false1['confidence'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglas_false1.sort_values(by=['confidence'], ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un gráfico donde mostremos las confianzas que tienen true y false\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bin_edges = np.arange(0, 1, 0.05)\n",
    "ax.hist(reglas_true1['confidence'], bins=bin_edges, label='TRUE', rwidth=0.85, color='#6c78d4')\n",
    "ax.hist(reglas_false1['confidence'], bins=bin_edges, label='FALSE', rwidth=0.85, color='#faed57')\n",
    "ax.set_xlabel('Confianza')\n",
    "ax.set_ylabel('Frecuencia')\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.gca().yaxis.grid(True, which='major')\n",
    "plt.gca().set_xticks(np.arange(0, 1, 0.05))\n",
    "plt.gca().set_yticks(np.arange(0, 130, 20))\n",
    "ax.set_title('Confianza de las reglas que contienen TRUE y FALSE')\n",
    "ax.legend(loc='upper right')\n",
    "plt.savefig('../images/confidence_rules.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reglas_true1))\n",
    "print(len(reglas_false1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos las 10 reglas que tienen mayor confianza\n",
    "reglas_VoF1.sort_values(by=['confidence'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos 30 consecuentes con reglas de confianza mayores y solo tienen un item en el consecuente\n",
    "lista_consecuentes = reglas_unigramas[(reglas_unigramas['confidence'] >= 0.9) & (reglas_unigramas['confidence'] <= 1) & (reglas_unigramas['consequents'].apply(lambda x: len(x) == 1))].sort_values(by=['confidence'], ascending=False)['consequents'].tolist()\n",
    "lista_consecuentes\n",
    "\n",
    "# La lista anterior repite items. Quiero los 30 items primeros unicos\n",
    "lista_consecuentes = list(set(lista_consecuentes))\n",
    "lista_consecuentes = lista_consecuentes[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reglas que no tienen TRUE\n",
    "reglas_sin_true = reglas_unigramas[~reglas_unigramas['antecedents'].apply(lambda x: 'TRUE' in x)]\n",
    "reglas_sin_true = reglas_sin_true[~reglas_unigramas['consequents'].apply(lambda x: 'TRUE' in x)]\n",
    "\n",
    "# Le quitamos las reglas que tengan new en el consecuente\n",
    "reglas_sin_true = reglas_sin_true[~reglas_sin_true['consequents'].apply(lambda x: 'new' in x)]\n",
    "reglas_sin_true = reglas_sin_true[~reglas_sin_true['consequents'].apply(lambda x: 'time' in x)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos de reglas_unigramas las 30 reglas con mayor confianza con un consecuente que pertenece a la lista anterior y un antecedente y TRUE no esta en los antecedentes\n",
    "reglas11 = reglas_sin_true[(reglas_sin_true['antecedents'].apply(lambda x: frozenset({'TRUE'}) not in x)) & (reglas_sin_true['confidence'] >= 0.9) & (reglas_sin_true['consequents'].apply(lambda x: x in lista_consecuentes)) & (reglas_sin_true['antecedents'].apply(lambda x: len(x) == 1))].sort_values(by=['confidence'], ascending=False).head(30)\n",
    "reglas21 = reglas_sin_true[(reglas_sin_true['antecedents'].apply(lambda x: frozenset({'TRUE'}) not in x)) & (reglas_sin_true['confidence'] >= 0.9) & (reglas_sin_true['consequents'].apply(lambda x: x in lista_consecuentes)) & (reglas_sin_true['antecedents'].apply(lambda x: len(x) == 2))].sort_values(by=['confidence'], ascending=False).head(30)\n",
    "reglas31 = reglas_sin_true[(reglas_sin_true['antecedents'].apply(lambda x: frozenset({'TRUE'}) not in x)) & (reglas_sin_true['confidence'] >= 0.9) & (reglas_sin_true['consequents'].apply(lambda x: x in lista_consecuentes)) & (reglas_sin_true['antecedents'].apply(lambda x: len(x) == 3))].sort_values(by=['confidence'], ascending=False).head(30)\n",
    "reglas41 = reglas_sin_true[(reglas_sin_true['antecedents'].apply(lambda x: frozenset({'TRUE'}) not in x)) & (reglas_sin_true['confidence'] >= 0.9) & (reglas_sin_true['consequents'].apply(lambda x: x in lista_consecuentes)) & (reglas_sin_true['antecedents'].apply(lambda x: len(x) == 4))].sort_values(by=['confidence'], ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglasn1 = reglas41.append(reglas31).append(reglas21).append(reglas11)\n",
    "reglasn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglasn1_formateadas = []\n",
    "for index, regla in reglasn1.iterrows():\n",
    "\tantecedentes = list(regla['antecedents'])\n",
    "\tconsecuente = list(regla['consequents'])[0]\n",
    "\tconfianza = regla['confidence']\n",
    "\tsoporte = regla['support']\n",
    "\treglasn1_formateadas.append((antecedentes, consecuente, confianza, soporte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglas_maximales = []\n",
    "\n",
    "for regla in reglasn1_formateadas:\n",
    "    antecedentes = regla[0]\n",
    "    consecuente = regla[1]\n",
    "    confianza = regla[2]\n",
    "    soporte = regla[3]\n",
    "    es_maximal = True\n",
    "    for regla2 in reglasn1_formateadas:\n",
    "        antecedentes_comparacion = regla2[0]\n",
    "        consecuente_comparacion = regla2[1]\n",
    "        if (consecuente == consecuente_comparacion and set(antecedentes).issubset(set(antecedentes_comparacion)) and set(antecedentes) != set(antecedentes_comparacion)):\n",
    "            es_maximal = False\n",
    "            break\n",
    "    \n",
    "    if es_maximal:\n",
    "        reglas_maximales.append((antecedentes, consecuente, confianza, soporte))\n",
    "\n",
    "print(reglas_maximales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos las reglas con más confianza de reglas_maximales\n",
    "reglas_maximales.sort(key=lambda x: x[2], reverse=True)\n",
    "reglas_maximales_ordenadas = reglas_maximales[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.DataFrame(reglas_maximales_ordenadas, columns=['antecedentes', 'consecuente', 'confianza', 'soporte'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.sort_values(by=['soporte'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mostramos minimo y maximo soporte\n",
    "print(\"Minimo soporte: \", df_tmp['confianza'].min())\n",
    "print(\"Maximo soporte: \", df_tmp['confianza'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear listas para los puntos en los ejes X e Y\n",
    "eje_x = []\n",
    "eje_y = []\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Iterar sobre las reglas\n",
    "for i, (antecedentes, consecuente, confianza, soporte) in enumerate(reglas_maximales_ordenadas):\n",
    "    num_antecedentes = len(antecedentes)\n",
    "    \n",
    "    # Crear puntos para los antecedentes en orden invertido\n",
    "    for j, antecedente in enumerate(reversed(antecedentes)):\n",
    "        eje_x.append(f'{num_antecedentes - j}')  # Cambiar el nombre en el eje X\n",
    "        eje_y.append(antecedente)\n",
    "    \n",
    "    # Crear punto para el consecuente\n",
    "    eje_x.append('consecuente')\n",
    "    eje_y.append(consecuente)\n",
    "    \n",
    "    # Conectar los antecedentes si hay más de uno\n",
    "    if num_antecedentes > 1:\n",
    "        for j in range(num_antecedentes-1):\n",
    "            ax.plot([f'{num_antecedentes - j}', f'{num_antecedentes - j - 1}'], \n",
    "                     [antecedentes[-j-1], antecedentes[-j-2]], linewidth=(confianza-0.9)/0.1, linestyle='-', color='orange')\n",
    "            \n",
    "    # Conexión entre el último antecedente y el consecuente\n",
    "    ax.plot([f'1', 'consecuente'], [antecedentes[0], consecuente], linewidth=0.5, linestyle='-', color='orange')\n",
    "    \n",
    "    \n",
    "# Graficar los puntos\n",
    "ax.scatter(eje_x, eje_y, s=1, marker='o')\n",
    "\n",
    "# Establecer límites de los ejes\n",
    "plt.ylim(-0.5, 38)\n",
    "\n",
    "# Ponemos lineas verticales en los xsticks\n",
    "for i in range(1, 5):\n",
    "    plt.axvline(x=f'{i}', color='gray', linestyle='--', linewidth=0.5)\n",
    "plt.axvline(x=f'consecuente', color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Añadimos una leyenda que explique el tamaño de las lineas\n",
    "for i in np.arange(0.925, 1.001, 0.025):\n",
    "    plt.plot([], [], color='orange', linewidth=(i-0.9)/0.1, label=f'Confianza={i:.3f}')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.savefig('../images/reglas_maximales.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un gráfico tipo matriz en el que cada columna sea un conjunto de antecedentes y cada fila un consecuente. Pondremos un punto que varíe su grosor según la confianza de la regla en las coordenadas (antecedentes,consecuente)\n",
    "# Creamos un diccionario con los antecedentes como claves y los consecuentes, confianzas y soportes en una lista\n",
    "lista__antecedente = []\n",
    "lista__consecuente = []\n",
    "lista__confianza = []\n",
    "lista__soporte = []\n",
    "\n",
    "# Crear listas para los puntos en los ejes X e Y\n",
    "eje_x = []\n",
    "eje_y = []\n",
    "\n",
    "for regla in reglas_maximales_ordenadas:\n",
    "\tlista__antecedente.append(' '.join(regla[0]))\n",
    "\tlista__consecuente.append(regla[1])\n",
    "\tlista__confianza.append(regla[2])\n",
    "\tlista__soporte.append(regla[3])\n",
    "reglas_maximales_dict = {'antecedentes': lista__antecedente, 'consecuente': lista__consecuente, 'confianza': lista__confianza, 'soporte': lista__soporte}\n",
    "\n",
    "# Creamos un DataFrame con el diccionario anterior\n",
    "df_reglas_maximales = pd.DataFrame(reglas_maximales_dict)\n",
    "df_reglas_maximales = df_reglas_maximales.reset_index(drop=True)\n",
    "\n",
    "# Creamos el gráfico iterando sobre cada fila\n",
    "fig, ax = plt.subplots(figsize=(5, 10))\n",
    "for index, regla in df_reglas_maximales.iterrows():\n",
    "    \n",
    "    # Crear punto para la regla\n",
    "\teje_x.append(regla['antecedentes'])\n",
    "\teje_y.append(regla['consecuente'])\n",
    "    \n",
    "# Graficar los puntos poniendo que el tamaño dependa del soporte y la transparencia de la confianza\n",
    "ax.scatter(eje_y, eje_x, s=df_reglas_maximales['soporte']*100, alpha=(df_reglas_maximales['confianza']-0.8)/0.2, marker='o', color='orange')\n",
    "\n",
    "# Añadimos una leyenda que explique el tamaño de los puntos\n",
    "for i in np.arange(0.825, 1.001, 0.025):\n",
    "\tplt.plot([], [], color='orange', alpha=min((i-0.8)/0.2,1), label=f'Confianza={i:.3f}', marker='o', linestyle='None', markersize=5)\n",
    "plt.legend()\n",
    "\n",
    "# Ponemos lineas verticales y horizontales en todos los sticks ya existentes por igual a la vez\n",
    "plt.grid(axis='both', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Rotamos las etiquetas del eje X\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Establecer límites de los ejes\n",
    "plt.ylim(-0.5, 39)\n",
    "plt.savefig('../images/reglas_maximales2.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso 2: N-Gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos get_grams\n",
    "def get_ngrams(text, n):\n",
    "\tn_grams = ngrams(text, n)\n",
    "\treturn [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# Creamos un nuevo dataframe donde para cada noticia almacenamos los ngramas de 1 a 3 de longitud\n",
    "df_train_ngramas = pd.DataFrame()\n",
    "df_train_ngramas['text'] = df_train['text']\n",
    "df_train_ngramas['ngrams'] = df_train_ngramas['text'].apply(lambda x: get_ngrams(x, 1)) + df_train_ngramas['text'].apply(lambda x: get_ngrams(x, 2)) + df_train_ngramas['text'].apply(lambda x: get_ngrams(x, 3))\n",
    "\n",
    "df_test_ngramas = pd.DataFrame()\n",
    "df_test_ngramas['text'] = df_test['text']\n",
    "df_test_ngramas['label']\t= df_test['label']\n",
    "df_test_ngramas['ngrams'] = df_test_ngramas['text'].apply(lambda x: get_ngrams(x, 1)) + df_test_ngramas['text'].apply(lambda x: get_ngrams(x, 2)) + df_test_ngramas['text'].apply(lambda x: get_ngrams(x, 3))\n",
    "\n",
    "# Descartamos los ngramas que incluyan las palabras 'TRUE' o 'FALSE'\n",
    "df_train_ngramas['ngrams'] = df_train_ngramas['ngrams'].apply(lambda x: [ngram for ngram in x if 'TRUE' not in ngram and 'FALSE' not in ngram])\n",
    "df_test_ngramas['ngrams'] = df_test_ngramas['ngrams'].apply(lambda x: [ngram for ngram in x if 'TRUE' not in ngram and 'FALSE' not in ngram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos cada conjunto de ngramas de una noticia en un set\n",
    "df_train_ngramas['ngrams'] = df_train_ngramas['ngrams'].apply(lambda x: set(x))\n",
    "df_test_ngramas['ngrams'] = df_test_ngramas['ngrams'].apply(lambda x: set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un set con todos los ngramas de train\n",
    "ngramas = []\n",
    "for i in range(len(df_train_ngramas['ngrams'])):\n",
    "\tngramas += df_train_ngramas['ngrams'].iloc[i]\n",
    "ngramas_set = set(ngramas)\n",
    "len(ngramas_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borramos objetos para liberar memoria\n",
    "del i\n",
    "del ngramas\n",
    "del df_clean\n",
    "del df_clean_tokens\n",
    "# del df_test\n",
    "# del df_test_ngramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos la frecuencia de cada ngrama en el conjunto de train como antes\n",
    "ngramas_counter = defaultdict(int)\n",
    "for index, noticia in df_train_ngramas.iterrows():\n",
    "\tfor ngr in noticia['ngrams']:\n",
    "\t\tngramas_counter[ngr] += 1\n",
    "\n",
    "# Extraemos en una lista los ngramas que aparecen menos de 'umbral' veces\n",
    "umbral = 50\n",
    "ngramas_eliminar = []\n",
    "for ngrama in ngramas_counter:\n",
    "\tif ngramas_counter[ngrama] < umbral:\n",
    "\t\tngramas_eliminar.append(ngrama)\n",
    "\n",
    "# Calculamos cuáles no vamos a eliminar, restando los conjuntos\n",
    "ngramas_no_eliminar = ngramas_set - set(ngramas_eliminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos el número total de ngramas en cada dataframe\n",
    "print(\"Número total de ngramas en el dataframe original: \", len(ngramas_counter))\n",
    "print(\"Número total de ngramas en el dataframe limpio: \", len(ngramas_no_eliminar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un nuevo dataframe donde eliminamos los ngramas que aparecen menos de 'umbral' veces\n",
    "df_train_ngramas_clean = pd.DataFrame()\n",
    "df_train_ngramas_clean['text'] = df_train_ngramas['text']\n",
    "df_train_ngramas_clean['ngrams'] = df_train_ngramas['ngrams'].apply(lambda x: set(x).intersection(ngramas_no_eliminar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con los ngramas maximales, es decir los ngramas que no están contenidos en otros ngramas\n",
    "ngramas_maximales = set(ngramas_no_eliminar)\n",
    "for ngrama in ngramas_no_eliminar:\n",
    "\tfor ngrama2 in ngramas_no_eliminar:\n",
    "\t\tif ngrama != ngrama2 and ngrama in ngrama2:\n",
    "\t\t\tngramas_maximales.discard(ngrama)\n",
    "\t\t\tbreak\n",
    "\n",
    "print(\"Número total de ngramas maximales: \", len(ngramas_maximales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramas_maximales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos variables para liberar memoria\n",
    "del df_train\n",
    "del index\n",
    "del ngr\n",
    "del ngramas_eliminar\n",
    "del ngramas_no_eliminar\n",
    "del ngramas_set\n",
    "del noticia\n",
    "del ngramas_counter\n",
    "del ngrama\n",
    "del ngrama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un set que contenga los bigramas incluidos en los ngramas maximales\n",
    "bigramas_maximales = set()\n",
    "for ngrama in ngramas_maximales:\n",
    "\tsplit = ngrama.split()\n",
    "\tif len(split) == 2:\n",
    "\t\tbigramas_maximales.add(ngrama)\n",
    "\telif len(split) > 2:\n",
    "\t\tfor i in range(len(split)-1):\n",
    "\t\t\tbigramas_maximales.add(split[i] + ' ' + split[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una lista de transacciones donde cada transacción es una lista de elementos\n",
    "transactions = df_train_ngramas_clean['text'].tolist()\n",
    "\n",
    "# Creamos un TransactionEncoder y lo ajustamos a la lista de transacciones\n",
    "te = TransactionEncoder()\n",
    "te.fit(transactions)\n",
    "\n",
    "# Transformamos las transacciones en una matriz booleana donde las filas representan las transacciones y las columnas representan los elementos\n",
    "data = te.transform(transactions)\n",
    "\n",
    "# Creamos un nuevo DataFrame a partir de la matriz booleana y las columnas de elementos correspondientes\n",
    "df_encoded = pd.DataFrame(data, columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los itemsets frecuentes\n",
    "freq_itemsets = apriori(df_encoded, min_support=0.2, use_colnames=True, verbose=1)\n",
    "freq_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reglas de asociación\n",
    "reglas = association_rules(freq_itemsets, metric=\"confidence\", min_threshold=0.1)\n",
    "reglas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escogemos las reglas que contienen un elemento en el antecedente y otro en el consecuente\n",
    "reglas_unigramas = reglas[(reglas['antecedents'].apply(lambda x: len(x) == 1)) & (reglas['consequents'].apply(lambda x: len(x) == 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una función que diga si una cadena de caracteres es un substring de un ngrama maximal\n",
    "def is_in_maximal(cadena):\n",
    "\treturn cadena in bigramas_maximales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escogemos las reglas cuyo antecedente y consecuente conjuntamente pertenecen a algun ngrama maximal\n",
    "reglas_intermedias = reglas_unigramas[\treglas_unigramas.apply(lambda x: is_in_maximal(list(x['antecedents'])[0] + ' ' + list(x['consequents'])[0]), axis=1)\t]\n",
    "\n",
    "# Escogemos las reglas que tengan minimo 0.8 de confianza\n",
    "#reglas_intermedias = reglas_intermedias[reglas_intermedias['confidence'] >= 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglas_intermedias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las palabras maximales como aquellas que se encuentran en los ngramas maximales\n",
    "palabras_maximales = set()\n",
    "for ngrama in ngramas_maximales:\n",
    "\tsplit = ngrama.split()\n",
    "\tfor palabra in split:\n",
    "\t\tpalabras_maximales.add(palabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escogemos las reglas que contienen solamente True o solamente False en el consecuente, y en el antecedente hay una palabra contenida en un ngrama maximal\n",
    "reglas_V = reglas_unigramas[reglas_unigramas['consequents'] == frozenset({'TRUE'})]\n",
    "reglas_V = reglas_V[reglas_V.apply(lambda x: list(x['antecedents'])[0] in palabras_maximales, axis=1)]\n",
    "reglas_V.sort_values(by=['confidence'], ascending=False)\n",
    "reglas_F = reglas_unigramas[reglas_unigramas['consequents'] == frozenset({'FALSE'})]\n",
    "reglas_F = reglas_F[reglas_F.apply(lambda x: list(x['antecedents'])[0] in palabras_maximales, axis=1)]\n",
    "reglas_F.sort_values(by=['confidence'], ascending=False)\n",
    "\n",
    "# Agrupamos las reglas verdaderas y falsas en un solo dataframe\n",
    "reglas_VoF = pd.DataFrame()\n",
    "reglas_VoF = reglas_V.append(reglas_F)\n",
    "reglas_VoF\n",
    "reglas_VoF.sort_values(by=['confidence'], ascending=False)\n",
    "\n",
    "# Escogemos reglas que tengan minimo 0.6 de confianza\n",
    "reglas_VoF = reglas_VoF[reglas_VoF['confidence'] >= 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una función que vea si hay una regla en reglas_intermedias con el antecedente y consecuente que le pasamos\n",
    "def is_in_reglas_intermedias(antecedente, consecuente):\n",
    "\treturn (antecedente, consecuente) in reglas_intermedias.apply(lambda x: (list(x['antecedents'])[0], list(x['consequents'])[0]), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escogemos las palabras conectadas con True o False\n",
    "palabras_conectadas = set()\n",
    "nuevas = set([list(x)[0] for x in reglas_VoF['antecedents']])\n",
    "while len(nuevas) > 0:\n",
    "    palabras_conectadas.update(nuevas)\n",
    "    palabras_conectadas_aux = nuevas.copy()\n",
    "    nuevas = set()\n",
    "    for ant in palabras_conectadas_aux:\n",
    "        nuevas.update(set(reglas_intermedias[reglas_intermedias.apply(lambda x: list(x['consequents'])[0] == ant, axis=1)]['antecedents'].apply(lambda x: list(x)[0]).tolist()))\n",
    "    nuevas.difference_update(palabras_conectadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(palabras_conectadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con las reglas donde tanto el antecedente como el consecuente son palabras conectadas\n",
    "reglas_conectadas = reglas_intermedias[reglas_intermedias.apply(lambda x: list(x['antecedents'])[0] in palabras_conectadas and list(x['consequents'])[0] in palabras_conectadas, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reglas_conectadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(reglas_conectadas['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(reglas_conectadas['lift'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglas_conectadas.iloc[0]['antecedents']\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos cuantas hay de cada una, y la confianza máxima y mínima de cada una\n",
    "print(\"Número de reglas con TRUE en el consecuente: \", len(reglas_VoF))\n",
    "print(\"Número de reglas con FALSE en el consecuente: \", len(reglas_VoF))\n",
    "print(\"Confianza máxima de las reglas que contienen TRUE: \", reglas_V['confidence'].max())\n",
    "print(\"Confianza mínima de las reglas que contienen TRUE: \", reglas_V['confidence'].min())\n",
    "print(\"Confianza máxima de las reglas que contienen FALSE: \", reglas_F['confidence'].max())\n",
    "print(\"Confianza mínima de las reglas que contienen FALSE: \", reglas_F['confidence'].min())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafo de Asociación para TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimales = 2\n",
    "\n",
    "# Seleccionamos las reglas que tienen una confianza grande\n",
    "reglas_VoF_grafo = reglas_VoF\n",
    "reglas_conectadas_grafo = reglas_conectadas\n",
    "\n",
    "# Calcular la distancia en aristas de cada nodo al nodo \"TRUE\"\n",
    "distancias = nx.shortest_path_length(G, target='TRUE')\n",
    "\n",
    "# Creamos un grafo de las reglas, donde los nodos son los elementos y las aristas son las reglas, y el valor de la arista es la confianza\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Añadimos los nodos\n",
    "for i in range(len(reglas_conectadas_grafo)):\n",
    "    G.add_node(next(iter(reglas_conectadas_grafo['antecedents'].iloc[i])))\n",
    "    G.add_node(next(iter(reglas_conectadas_grafo['consequents'].iloc[i])))\n",
    "\n",
    "for i in range(len(reglas_VoF_grafo)):\n",
    "    G.add_node(next(iter(reglas_VoF_grafo['antecedents'].iloc[i])))\n",
    "    G.add_node(next(iter(reglas_VoF_grafo['consequents'].iloc[i])))\n",
    "\n",
    "# Añadimos las aristas con la confianza como peso\n",
    "for i in range(len(reglas_conectadas_grafo)):\n",
    "    G.add_edge(next(iter(reglas_conectadas_grafo['antecedents'].iloc[i])),\n",
    "               next(iter(reglas_conectadas_grafo['consequents'].iloc[i])),\n",
    "               weight=math.floor(reglas_conectadas_grafo['confidence'].iloc[i] * 10 ** decimales) / 10 ** decimales)\n",
    "\n",
    "for i in range(len(reglas_VoF_grafo)):\n",
    "    G.add_edge(next(iter(reglas_VoF_grafo['antecedents'].iloc[i])),\n",
    "               next(iter(reglas_VoF_grafo['consequents'].iloc[i])),\n",
    "               weight=math.floor(reglas_VoF_grafo['confidence'].iloc[i] * 10 ** decimales) / 10 ** decimales)\n",
    "\n",
    "# Eliminar nodos sin aristas asociadas\n",
    "isolated_nodes = [node for node in G.nodes if G.degree[node] == 0]\n",
    "G.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "# Dibujamos el grafo\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Convertimos las posiciones a tuplas\n",
    "pos = nx.spring_layout(G, k=1.5, weight='weight')\n",
    "pos = {node: tuple(coord) for node, coord in pos.items()}\n",
    "\n",
    "# Ordenar los nodos por distancia al nodo \"TRUE\"\n",
    "sorted_nodes = sorted(G.nodes, key=lambda x: distancias[x])\n",
    "\n",
    "# Asignar posiciones a los nodos según las distancias\n",
    "pos_final = {}\n",
    "x_spacing = 1.5  # Espaciado horizontal entre los nodos\n",
    "y_spacing = 1.0  # Espaciado vertical entre los niveles\n",
    "y_offset = 0.0\n",
    "\n",
    "primera_distancia_1 = True\n",
    "primera_distancia_2 = True\n",
    "for nodo in sorted_nodes:\n",
    "    distancia = distancias[nodo]\n",
    "    x_offset = x_spacing * distancia\n",
    "    if distancia == 0:\n",
    "        y_offset = 10\n",
    "    elif distancia == 1 and primera_distancia_1:\n",
    "        primera_distancia_1 = False\n",
    "        y_offset = 3\n",
    "    elif distancia == 1 and not primera_distancia_1:\n",
    "        y_offset += 3\n",
    "    elif distancia == 2 and primera_distancia_2:\n",
    "        primera_distancia_2 = False\n",
    "        y_offset = 3\n",
    "    elif distancia == 2 and not primera_distancia_2:\n",
    "        y_offset += 3\n",
    "    pos_final[nodo] = (x_offset, y_offset)\n",
    "\n",
    "# Dibujamos los nodos en diferentes colores según su nivel\n",
    "for node in G.nodes:\n",
    "    if distancias[node] == 0:\n",
    "        nx.draw_networkx_nodes(G, pos_final, nodelist=[node], node_size=2000, node_color='#ffead2')\n",
    "    elif distancias[node] == 1:\n",
    "        nx.draw_networkx_nodes(G, pos_final, nodelist=[node], node_size=1000, node_color='#8294c4')\n",
    "    elif distancias[node] == 2:\n",
    "        nx.draw_networkx_nodes(G, pos_final, nodelist=[node], node_size=1000, node_color='#dbdfea')\n",
    "\n",
    "# Dibujamos las aristas curvadas entre nodos del mismo nivel\n",
    "same_level_edges = [(u, v, d) for u, v, d in G.edges(data=True) if distancias[u] == distancias[v]]\n",
    "different_level_edges = [(u, v, d) for u, v, d in G.edges(data=True) if distancias[u] != distancias[v]]\n",
    "\n",
    "nx.draw_networkx_edges(G, pos_final, edgelist=same_level_edges, connectionstyle=\"arc3,rad=1\", alpha=0.5, edge_color='black')\n",
    "nx.draw_networkx_edges(G, pos_final, edgelist=different_level_edges, alpha=0.5, edge_color='black')\n",
    "\n",
    "nx.draw_networkx_labels(G, pos_final, font_size=20, font_color='black')\n",
    "\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "# Ajustamos la posición de las etiquetas a lo largo de las aristas curvadas\n",
    "curved_edge_labels = {}\n",
    "for edge, weight in edge_labels.items():\n",
    "    if edge in same_level_edges:\n",
    "        u, v = edge[:2]\n",
    "        x1, y1 = pos_final[u]\n",
    "        x2, y2 = pos_final[v]\n",
    "        label_pos = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "        curved_edge_labels[edge] = {'weight': weight, 'label_pos': label_pos}\n",
    "    else:\n",
    "        curved_edge_labels[edge] = weight\n",
    "\n",
    "nx.draw_networkx_edge_labels(G, pos_final, edge_labels=curved_edge_labels, font_size=20, bbox=dict(facecolor='none', edgecolor='none'))\n",
    "plt.savefig('../images/reglas_maximales3.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimales = 2\n",
    "\n",
    "# Seleccionamos las reglas que tienen una confianza grande\n",
    "reglas_VoF_grafo = reglas_VoF\n",
    "reglas_conectadas_grafo = reglas_conectadas\n",
    "\n",
    "# Creamos un grafo de las reglas, donde los nodos son los elementos y las aristas son las reglas, y el valor de la arista es la confianza\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Añadimos los nodos\n",
    "for i in range(len(reglas_conectadas_grafo)):\n",
    "    G.add_node(next(iter(reglas_conectadas_grafo['antecedents'].iloc[i])))\n",
    "    G.add_node(next(iter(reglas_conectadas_grafo['consequents'].iloc[i])))\n",
    "\n",
    "for i in range(len(reglas_VoF_grafo)):\n",
    "    G.add_node(next(iter(reglas_VoF_grafo['antecedents'].iloc[i])))\n",
    "    G.add_node(next(iter(reglas_VoF_grafo['consequents'].iloc[i])))\n",
    "\n",
    "# Añadimos las aristas con la confianza como peso\n",
    "for i in range(len(reglas_conectadas_grafo)):\n",
    "    G.add_edge(next(iter(reglas_conectadas_grafo['antecedents'].iloc[i])),\n",
    "               next(iter(reglas_conectadas_grafo['consequents'].iloc[i])),\n",
    "               weight=math.floor(reglas_conectadas_grafo['confidence'].iloc[i] * 10 ** decimales) / 10 ** decimales)\n",
    "\n",
    "for i in range(len(reglas_VoF_grafo)):\n",
    "    G.add_edge(next(iter(reglas_VoF_grafo['antecedents'].iloc[i])),\n",
    "               next(iter(reglas_VoF_grafo['consequents'].iloc[i])),\n",
    "               weight=math.floor(reglas_VoF_grafo['confidence'].iloc[i] * 10 ** decimales) / 10 ** decimales)\n",
    "\n",
    "# Asignamos colores a los nodos según su nivel\n",
    "colores = []\n",
    "for node in G.nodes:\n",
    "    if distancias[node] == 0:\n",
    "        colores.append('#ffead2')\n",
    "    elif distancias[node] == 1:\n",
    "        colores.append('#8294c4')\n",
    "    elif distancias[node] == 2:\n",
    "        colores.append('#dbdfea')\n",
    "\n",
    "# Dibujamos el grafo\n",
    "plt.figure(figsize=(20, 14))\n",
    "pos = nx.spring_layout(G, k=1, weight='weight')  # Ajusta el valor de k según tus necesidades\n",
    "nx.draw(G, pos, with_labels=True, font_size=20, node_size=2000, node_color='lightblue', edge_color='black', width=1)\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=20)\n",
    " # Dibujamos el grafo añadiendo los colores\n",
    "nx.draw(G, pos, with_labels=True, font_size=20, node_size=2000, node_color=colores, edge_color='black', width=1)\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=20,bbox=dict(facecolor='none', edgecolor='none'))\n",
    "\n",
    "plt.savefig('../images/reglas_maximales4.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención reglas FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escogemos las palabras conectadas con solo False\n",
    "palabras_conectadas = set()\n",
    "nuevas = set([list(x)[0] for x in reglas_F['antecedents']])\n",
    "while len(nuevas) > 0:\n",
    "    palabras_conectadas.update(nuevas)\n",
    "    palabras_conectadas_aux = nuevas.copy()\n",
    "    nuevas = set()\n",
    "    for ant in palabras_conectadas_aux:\n",
    "        nuevas.update(set(reglas_intermedias[reglas_intermedias.apply(lambda x: list(x['consequents'])[0] == ant, axis=1)]['antecedents'].apply(lambda x: list(x)[0]).tolist()))\n",
    "    nuevas.difference_update(palabras_conectadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(palabras_conectadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con las reglas donde tanto el antecedente como el consecuente son palabras conectadas\n",
    "reglas_conectadas = reglas_intermedias[reglas_intermedias.apply(lambda x: list(x['antecedents'])[0] in palabras_conectadas and list(x['consequents'])[0] in palabras_conectadas, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reglas_conectadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(reglas_conectadas['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(reglas_conectadas['lift'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos la interseccion entre palabras_conectadas y los antecedentes de reglas_F\n",
    "len(set(reglas_F['antecedents'].apply(lambda x: list(x)[0]).tolist()).intersection(palabras_conectadas))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafo de Asociación para FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimales = 2\n",
    "\n",
    "# Seleccionamos las reglas\n",
    "reglas_VoF_grafo = reglas_F\n",
    "reglas_conectadas_grafo = reglas_conectadas[reglas_conectadas['confidence'] >= 0.9]\n",
    "\n",
    "\n",
    "# Creamos un grafo de las reglas, donde los nodos son los elementos y las aristas son las reglas, y el valor de la arista es la confianza\n",
    "G = nx.DiGraph()\n",
    "\n",
    "\n",
    "# Añadimos los nodos\n",
    "for i in range(len(reglas_conectadas_grafo)):\n",
    "    G.add_node(next(iter(reglas_conectadas_grafo['antecedents'].iloc[i])))\n",
    "    G.add_node(next(iter(reglas_conectadas_grafo['consequents'].iloc[i])))\n",
    "\n",
    "for i in range(len(reglas_VoF_grafo)):\n",
    "    G.add_node(next(iter(reglas_VoF_grafo['antecedents'].iloc[i])))\n",
    "    G.add_node(next(iter(reglas_VoF_grafo['consequents'].iloc[i])))\n",
    "    \n",
    "\n",
    "# Añadimos las aristas con la confianza como peso\n",
    "for i in range(len(reglas_conectadas_grafo)):\n",
    "    G.add_edge(next(iter(reglas_conectadas_grafo['antecedents'].iloc[i])),\n",
    "               next(iter(reglas_conectadas_grafo['consequents'].iloc[i])),\n",
    "               weight=math.floor(reglas_conectadas_grafo['confidence'].iloc[i] * 10 ** decimales) / 10 ** decimales)\n",
    "\n",
    "for i in range(len(reglas_VoF_grafo)):\n",
    "    G.add_edge(next(iter(reglas_VoF_grafo['antecedents'].iloc[i])),\n",
    "               next(iter(reglas_VoF_grafo['consequents'].iloc[i])),\n",
    "               weight=math.floor(reglas_VoF_grafo['confidence'].iloc[i] * 10 ** decimales) / 10 ** decimales)\n",
    "\n",
    "# Calcular la distancia en aristas de cada nodo al nodo \"FALSE\"\n",
    "distancias = nx.shortest_path_length(G, target='FALSE')\n",
    "eliminar = []\n",
    "\n",
    "# Vemos los nodos que no están en distancias, y los borramos\n",
    "for node in G.nodes:\n",
    "    if node not in distancias.keys():\n",
    "        eliminar.append(node)        \n",
    "G.remove_nodes_from(eliminar)\n",
    "\n",
    "# Asignamos colores a los nodos según su nivel\n",
    "colores = []\n",
    "for node in G.nodes:\n",
    "    # Comprobamos si existe la distancia en el diccionario de distancias, ya que puede que no exista\n",
    "    if distancias.get(node) != None:\n",
    "        if distancias[node] == 0:\n",
    "            colores.append('#ffead2')\n",
    "        elif distancias[node] == 1:\n",
    "            colores.append('#8294c4')\n",
    "        elif distancias[node] == 2:\n",
    "            colores.append('#dbdfea')\n",
    "\n",
    "# Dibujamos el grafo\n",
    "plt.figure(figsize=(12, 8))\n",
    "pos = nx.spring_layout(G, k=1, weight='weight')  # Ajusta el valor de k según tus necesidades\n",
    "nx.draw(G, pos, with_labels=True, font_size=20, node_size=2000, node_color='lightblue', edge_color='black', width=1)\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=20, bbox=dict(facecolor='none', edgecolor='none'))\n",
    " # Dibujamos el grafo añadiendo los colores\n",
    "nx.draw(G, pos, with_labels=True, font_size=20, node_size=2000, node_color=colores, edge_color='black', width=1)\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=20,bbox=dict(facecolor='none', edgecolor='none'))\n",
    "\n",
    "plt.savefig('../images/reglas_maximales_false.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimales = 2\n",
    "\n",
    "# Seleccionamos las reglas\n",
    "reglas_VoF_grafo = reglas_F\n",
    "reglas_conectadas_grafo = reglas_conectadas[reglas_conectadas['confidence'] >= 0.9]\n",
    "\n",
    "# Calcular la distancia en aristas de cada nodo al nodo \"TRUE\"\n",
    "distancias = nx.shortest_path_length(G, target='FALSE')\n",
    "\n",
    "# Creamos un grafo de las reglas, donde los nodos son los elementos y las aristas son las reglas, y el valor de la arista es la confianza\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Añadimos los nodos\n",
    "for i in range(len(reglas_conectadas_grafo)):\n",
    "    G.add_node(next(iter(reglas_conectadas_grafo['antecedents'].iloc[i])))\n",
    "    G.add_node(next(iter(reglas_conectadas_grafo['consequents'].iloc[i])))\n",
    "\n",
    "for i in range(len(reglas_VoF_grafo)):\n",
    "    G.add_node(next(iter(reglas_VoF_grafo['antecedents'].iloc[i])))\n",
    "    G.add_node(next(iter(reglas_VoF_grafo['consequents'].iloc[i])))\n",
    "\n",
    "# Añadimos las aristas con la confianza como peso\n",
    "for i in range(len(reglas_conectadas_grafo)):\n",
    "    G.add_edge(next(iter(reglas_conectadas_grafo['antecedents'].iloc[i])),\n",
    "               next(iter(reglas_conectadas_grafo['consequents'].iloc[i])),\n",
    "               weight=math.floor(reglas_conectadas_grafo['confidence'].iloc[i] * 10 ** decimales) / 10 ** decimales)\n",
    "\n",
    "for i in range(len(reglas_VoF_grafo)):\n",
    "    G.add_edge(next(iter(reglas_VoF_grafo['antecedents'].iloc[i])),\n",
    "               next(iter(reglas_VoF_grafo['consequents'].iloc[i])),\n",
    "               weight=math.floor(reglas_VoF_grafo['confidence'].iloc[i] * 10 ** decimales) / 10 ** decimales)\n",
    "\n",
    "# Calcular la distancia en aristas de cada nodo al nodo \"FALSE\"\n",
    "distancias = nx.shortest_path_length(G, target='FALSE')\n",
    "eliminar = []\n",
    "\n",
    "# Vemos los nodos que no están en distancias, y los borramos\n",
    "for node in G.nodes:\n",
    "    if node not in distancias.keys():\n",
    "        eliminar.append(node)        \n",
    "G.remove_nodes_from(eliminar)\n",
    "\n",
    "# Dibujamos el grafo\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Convertimos las posiciones a tuplas\n",
    "pos = nx.spring_layout(G, k=1.5, weight='weight')\n",
    "pos = {node: tuple(coord) for node, coord in pos.items()}\n",
    "\n",
    "# Ordenar los nodos por distancia al nodo \"TRUE\"\n",
    "sorted_nodes = sorted(G.nodes, key=lambda x: distancias[x])\n",
    "\n",
    "# Asignar posiciones a los nodos según las distancias\n",
    "pos_final = {}\n",
    "x_spacing = 1.5  # Espaciado horizontal entre los nodos\n",
    "y_spacing = 1.0  # Espaciado vertical entre los niveles\n",
    "y_offset = 0.0\n",
    "\n",
    "primera_distancia_1 = True\n",
    "primera_distancia_2 = True\n",
    "for nodo in sorted_nodes:\n",
    "    distancia = distancias[nodo]\n",
    "    x_offset = x_spacing * distancia\n",
    "    if distancia == 0:\n",
    "        y_offset = 16\n",
    "    elif distancia == 1 and primera_distancia_1:\n",
    "        primera_distancia_1 = False\n",
    "        y_offset = 3\n",
    "    elif distancia == 1 and not primera_distancia_1:\n",
    "        y_offset += 3\n",
    "    elif distancia == 2 and primera_distancia_2:\n",
    "        primera_distancia_2 = False\n",
    "        y_offset = 3\n",
    "    elif distancia == 2 and not primera_distancia_2:\n",
    "        y_offset += 3\n",
    "    pos_final[nodo] = (x_offset, y_offset)\n",
    "\n",
    "# Dibujamos los nodos en diferentes colores según su nivel\n",
    "for node in G.nodes:\n",
    "    if distancias[node] == 0:\n",
    "        nx.draw_networkx_nodes(G, pos_final, nodelist=[node], node_size=2000, node_color='#ffead2')\n",
    "    elif distancias[node] == 1:\n",
    "        nx.draw_networkx_nodes(G, pos_final, nodelist=[node], node_size=1000, node_color='#8294c4')\n",
    "    elif distancias[node] == 2:\n",
    "        nx.draw_networkx_nodes(G, pos_final, nodelist=[node], node_size=1000, node_color='#dbdfea')\n",
    "\n",
    "# Dibujamos las aristas curvadas entre nodos del mismo nivel\n",
    "same_level_edges = [(u, v, d) for u, v, d in G.edges(data=True) if distancias[u] == distancias[v]]\n",
    "different_level_edges = [(u, v, d) for u, v, d in G.edges(data=True) if distancias[u] != distancias[v]]\n",
    "\n",
    "nx.draw_networkx_edges(G, pos_final, edgelist=same_level_edges, connectionstyle=\"arc3,rad=1\", alpha=0.5, edge_color='black')\n",
    "nx.draw_networkx_edges(G, pos_final, edgelist=different_level_edges, alpha=0.5, edge_color='black')\n",
    "\n",
    "nx.draw_networkx_labels(G, pos_final, font_size=20, font_color='black')\n",
    "\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "# Ajustamos la posición de las etiquetas a lo largo de las aristas curvadas\n",
    "curved_edge_labels = {}\n",
    "for edge, weight in edge_labels.items():\n",
    "    if edge in same_level_edges:\n",
    "        u, v = edge[:2]\n",
    "        x1, y1 = pos_final[u]\n",
    "        x2, y2 = pos_final[v]\n",
    "        label_pos = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "        curved_edge_labels[edge] = {'weight': weight, 'label_pos': label_pos}\n",
    "    else:\n",
    "        curved_edge_labels[edge] = weight\n",
    "\n",
    "nx.draw_networkx_edge_labels(G, pos_final, edge_labels=curved_edge_labels, label_pos=0.75, font_size=20, bbox=dict(facecolor='none', edgecolor='none'))\n",
    "plt.savefig('../images/reglas_maximales_false2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos si el grafo tiene ciclos\n",
    "nx.is_directed_acyclic_graph(G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee5bf18464c5a0cdd7c8ceb4ffa371b05ef38df8192946373c6ecb0c4072f62a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
